{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d80e21-2364-4d91-b9bf-b127fb831f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, month, to_date, date_format, lit\n",
    "from pyspark import SparkConf\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b501018-107f-43db-96c1-a9de9bf6f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get env vars\n",
    "database_name = os.environ.get(\"DB_NAME\")\n",
    "table_name = os.environ.get(\"TB_NAME\")\n",
    "bucket_name = os.environ.get(\"BUCKET_NAME\")\n",
    "work_group = os.environ.get(\"WORKGROUP\")\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "role_arn = os.environ.get(\"AWS_ROLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42acc170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the IAM role\n",
    "# Create a \"starter\" session with your existing credentials\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Assume the IAM role\n",
    "assumed_role_response = sts_client.assume_role(\n",
    "    RoleArn=role_arn,\n",
    "    RoleSessionName='SampleSessionName'\n",
    ")\n",
    "\n",
    "# # 3. Get the temporary credentials from the response\n",
    "# credentials = assumed_role_response['Credentials']\n",
    "\n",
    "# # 4. Create a new session with the assumed role credentials\n",
    "# session = boto3.Session(\n",
    "#     aws_access_key_id=credentials['AccessKeyId'],\n",
    "#     aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "#     aws_session_token=credentials['SessionToken']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff74108-e8ba-4fc3-b7dd-1b9b0d8f9a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "25/02/09 18:41:28 INFO SparkContext: Running Spark version 3.3.0-amzn-1\n",
      "25/02/09 18:41:29 INFO ResourceUtils: ==============================================================\n",
      "25/02/09 18:41:29 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/02/09 18:41:29 INFO ResourceUtils: ==============================================================\n",
      "25/02/09 18:41:29 INFO SparkContext: Submitted application: sample_spark_app\n",
      "25/02/09 18:41:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/02/09 18:41:29 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/02/09 18:41:29 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/02/09 18:41:29 INFO SecurityManager: Changing view acls to: glue_user\n",
      "25/02/09 18:41:29 INFO SecurityManager: Changing modify acls to: glue_user\n",
      "25/02/09 18:41:29 INFO SecurityManager: Changing view acls groups to: \n",
      "25/02/09 18:41:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/02/09 18:41:29 INFO SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(glue_user); groups with view permissions: Set(); users  with modify permissions: Set(glue_user); groups with modify permissions: Set()\n",
      "25/02/09 18:41:29 INFO Utils: Successfully started service 'sparkDriver' on port 38585.\n",
      "25/02/09 18:41:29 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/02/09 18:41:29 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/02/09 18:41:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/02/09 18:41:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/02/09 18:41:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/02/09 18:41:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-45534705-4a68-4d4e-92da-ca424aee2821\n",
      "25/02/09 18:41:29 INFO MemoryStore: MemoryStore started with capacity 6.2 GiB\n",
      "25/02/09 18:41:29 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/02/09 18:41:29 INFO SubResultCacheManager: Sub-result caches are disabled.\n",
      "25/02/09 18:41:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/02/09 18:41:30 INFO Executor: Starting executor ID driver on host 191e896d6d11\n",
      "25/02/09 18:41:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/home/glue_user/spark/jars/*,file:/home/glue_user/aws-glue-libs/jars/*,file:/home/glue_user/workspace/jupyter_workspace/*'\n",
      "25/02/09 18:41:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37285.\n",
      "25/02/09 18:41:30 INFO NettyBlockTransferService: Server created on 191e896d6d11:37285\n",
      "25/02/09 18:41:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/02/09 18:41:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 191e896d6d11, 37285, None)\n",
      "25/02/09 18:41:30 INFO BlockManagerMasterEndpoint: Registering block manager 191e896d6d11:37285 with 6.2 GiB RAM, BlockManagerId(driver, 191e896d6d11, 37285, None)\n",
      "25/02/09 18:41:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 191e896d6d11, 37285, None)\n",
      "25/02/09 18:41:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 191e896d6d11, 37285, None)\n",
      "25/02/09 18:41:30 INFO SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/local-1739126489983.inprogress\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('sample_spark_app') \\\n",
    "    .config(\"spark.sql.catalog.AwsGlueCatalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.AwsGlueCatalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.AwsGlueCatalog.warehouse\", \"s3a://bd-datawarehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.AwsGlueCatalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to WARN\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6b9ba8-2838-4f3d-bedf-2b2f4e4719af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample csv dataset\n",
    "df = spark.read.csv('data/tips.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c28fb3c-1f93-43bd-bfd1-fff724c31abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its just a good practice to add a 'created_at' column storing data timestamp. It also will be used in this sample as Iceberg partion in the form: month('created_at')\n",
    "# Create 'created_at' column and init\n",
    "df = df.withColumn('created_at', F.current_timestamp())\n",
    "\n",
    "# Just for testing purposes and in case you want to test iceberg partition behaviour inserting several months\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "# from datetime import datetime\n",
    "# df = df.withColumn('created_at', F.lit(datetime.now() + relativedelta(months=-1)).cast('timestamp'))\n",
    "\n",
    "# Correct way to reorder (and avoid duplicates):\n",
    "df = df.select(\"created_at\", *[col for col in df.columns if col != \"created_at\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573b7135-8e78-434d-b9f6-9a078cf1e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column regarding vat taxes of 21% \n",
    "df = df.withColumn('vat', F.col('total_bill') * 0.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcf7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'bd-datawarehouse' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_s3_bucket_if_not_exists(bucket_name, region_name=None):\n",
    "    try:\n",
    "        s3 = boto3.client('s3', region_name=region_name)\n",
    "        # Check if the bucket exists\n",
    "        try:\n",
    "            s3.head_bucket(Bucket=bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code'] \n",
    "            if error_code == '404':\n",
    "                print(f\"Bucket '{bucket_name}' does not exist. Creating...\")\n",
    "                if region_name:\n",
    "                    try:\n",
    "                        s3.create_bucket(\n",
    "                            Bucket=bucket_name,\n",
    "                            CreateBucketConfiguration={'LocationConstraint': region_name}\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating bucket in region {region_name}: {e}\")\n",
    "                        return False\n",
    "                else:\n",
    "                    try:\n",
    "                        s3.create_bucket(Bucket=bucket_name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating bucket: {e}\")\n",
    "                        return False\n",
    "                print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "                return True\n",
    "\n",
    "            else:\n",
    "                print(f\"An unexpected error occurred checking or creating bucket: {e}\")\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        print(f\"A general error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Create bucket if not exists.\n",
    "create_s3_bucket_if_not_exists(bucket_name, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "593e6285-909e-4a93-aeca-fa0cc3d2c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path already exists: s3://bd-datawarehouse/database_name/table_name/\n"
     ]
    }
   ],
   "source": [
    "# Define S3 bucket and path for database and table\n",
    "s3_path = f\"{database_name}/{table_name}/\"  # Path inside the bucket\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def s3_path_exists(bucket, path):\n",
    "    \"\"\"Check if an S3 path exists by listing objects with that prefix.\"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=path)\n",
    "    return \"Contents\" in response  # Returns True if objects exist\n",
    "\n",
    "def create_s3_path(bucket, path):\n",
    "    \"\"\"Create an empty directory in S3 by uploading an empty file.\"\"\"\n",
    "    if not s3_path_exists(bucket, path):\n",
    "        s3.put_object(Bucket=bucket, Key=f\"{path}placeholder.txt\", Body=b\"\")  # Upload an empty file\n",
    "        print(f\"Created path: s3://{bucket}/{path}\")\n",
    "    else:\n",
    "        print(f\"Path already exists: s3://{bucket}/{path}\")\n",
    "\n",
    "# Check and create the path if needed\n",
    "create_s3_path(bucket_name, s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ba1031-908e-4a41-b5de-214cf0a1d6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database database_name created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create database if not exists\n",
    "glue_client = boto3.client('glue', region_name=\"eu-west-1\")  # Change to your region\n",
    "\n",
    "database_location = f\"s3://{bucket_name}/{database_name}\"\n",
    "\n",
    "# Check if the database already exists\n",
    "existing_databases = [db['Name'] for db in glue_client.get_databases()['DatabaseList']]\n",
    "if database_name not in existing_databases:\n",
    "    glue_client.create_database(DatabaseInput={'Name': database_name, 'LocationUri': database_location})\n",
    "    print(f\"Database {database_name} created successfully.\")\n",
    "else:\n",
    "    print(f\"Database {database_name} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5b7031-8bc3-4740-8b61-d3bcab03ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to options in order to partition by month\n",
    "# Create a non hidden field in order to be used as partition field\n",
    "# df = df.withColumn(\"yearmonth\", date_format(F.col(\"created_at\"), \"yyyyMM\").cast(\"int\")) # or string\n",
    "# or just use a hidden partition with transform method, e.g.: \"PARTITIONED BY (month(created_at))\", check https://medium.com/@life-is-short-so-enjoy-it/aws-athena-iceberg-experiment-dropping-partitions-month-b5074e56c911 in order to understand what partitions month value means.\n",
    "# Iceberg table properties could be readed using: \"describe formatted <database_name>.<table_name>;\"\n",
    " #\n",
    "\n",
    "\n",
    "# Create Iceberg table if not exist partionted by created_at\n",
    "spark.sql(f\"\"\"    \n",
    "    CREATE TABLE IF NOT EXISTS AwsGlueCatalog.{database_name}.{table_name} (\n",
    "        created_at timestamp\n",
    "    )\n",
    "    PARTITIONED BY (month(created_at))\n",
    "    LOCATION 's3://{bucket_name}/{database_name}/{table_name}'\n",
    "    TBLPROPERTIES (\n",
    "        'table_type' = 'ICEBERG',\n",
    "        'format'='parquet',        \n",
    "        'write_compression'='ZSTD',        \n",
    "        'optimize_rewrite_data_file_threshold'='5',\n",
    "        'optimize_rewrite_delete_file_threshold'='2',\n",
    "        'vacuum_min_snapshots_to_keep'='5'\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccfab9d6-9b96-4df9-8cdc-d1a8e3b43e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# Write to table using awswrangler.athena.to_iceberg -> https://aws-sdk-pandas.readthedocs.io/en/3.2.1/stubs/awswrangler.athena.to_iceberg.html\n",
    "wr.athena.to_iceberg(\n",
    "        df=df.toPandas(),\n",
    "        database=database_name,\n",
    "        table=table_name,   \n",
    "        temp_path=f\"s3://{bucket_name}/{database_name}/tmp_table_{table_name}/\",\n",
    "        workgroup=work_group,\n",
    "        keep_files=False,\n",
    "        schema_evolution=True,\n",
    "        fill_missing_columns_in_df=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ab10c-f842-4868-9248-7ff1eb4047fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"DESCRIBE TABLE EXTENDED AwsGlueCatalog.{database_name}.{table_name}\")\n",
    "df.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ddc5b-55b0-4265-a357-15a68c43aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64283e3-8d33-4a3b-a280-4dd5915bca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *  # Import Spark functions\n",
    "\n",
    "# # Initialize a SparkSession\n",
    "# spark = SparkSession.builder.appName(\"IcebergExample\").getOrCreate()\n",
    "\n",
    "# # Configure Iceberg (replace with your actual configuration)\n",
    "# spark.conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "# spark.conf.set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.IcebergCatalog\")\n",
    "# spark.conf.set(\"spark.sql.catalog.iceberg.type\", \"hadoop\") # or hive\n",
    "# spark.conf.set(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://your-iceberg-warehouse\") # or hdfs://path\n",
    "\n",
    "# # Create a sample DataFrame\n",
    "# data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
    "# df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# # Write to Iceberg (create a new table or overwrite if it exists)\n",
    "# df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"iceberg.your_catalog.your_table\") # iceberg.your_catalog is required. your_table is the table name.\n",
    "\n",
    "# # Read from Iceberg\n",
    "# iceberg_df = spark.read.format(\"iceberg\").table(\"iceberg.your_catalog.your_table\") # Read from iceberg.your_catalog.your_table\n",
    "\n",
    "# iceberg_df.show()\n",
    "\n",
    "# # Example Iceberg queries\n",
    "# # You can use SQL queries to interact with Iceberg tables\n",
    "# spark.sql(\"SELECT * FROM iceberg.your_catalog.your_table WHERE age > 25\").show()\n",
    "\n",
    "# # Example of updating data in an Iceberg table\n",
    "# updatesDF = spark.createDataFrame([(\"Alice\", 26)], [\"name\", \"age\"]) # Create a dataframe with updates\n",
    "# updatesDF.write.format(\"iceberg\").mode(\"merge\").option(\"mergeSchema\", \"true\").saveAsTable(\"iceberg.your_catalog.your_table\") # Merge the updates\n",
    "\n",
    "# # Example of deleting data from an Iceberg table\n",
    "# df.filter(\"age > 27\").write.format(\"iceberg\").mode(\"delete\").saveAsTable(\"iceberg.your_catalog.your_table\")\n",
    "\n",
    "# # Show the updated table\n",
    "# iceberg_df = spark.read.format(\"iceberg\").table(\"iceberg.your_catalog.your_table\") # Read from iceberg.your_catalog.your_table\n",
    "# iceberg_df.show()\n",
    "\n",
    "# # Stop the SparkSession\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
